{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpmXygg_yoY_",
        "outputId": "c4275555-4203-450b-a95a-05ee272949e4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "login(token='YOUR_HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL7xsdMpvzNa"
      },
      "source": [
        "## Load model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "de7c64f08ac44d6c99440b0715946a95",
            "16cc667f790b404c998429efd29f3c3b",
            "44c66b45fbc143f69087b4186d707686",
            "bb953d96938947f59bcedb09d9da265d",
            "2d8c7f728e6a46fc9ec33dc8bcf46403",
            "a61dfba2af6345abb338698f39921d4b",
            "31b7978c8707428684ac4248ff851a81",
            "cd84083292304a21a053ef1aad860ae1",
            "18fd7e2110a548ff9da6ce1f558dcf29",
            "b3eaa6296a7846f7880c1ad79c49ba50",
            "7b9fdb818bee4f5f86b2c0e890967622"
          ]
        },
        "id": "VYBfqmRbv2NS",
        "outputId": "7f451803-8f09-48bf-e799-51df4aae3cd0"
      },
      "outputs": [],
      "source": [
        "# change this to any decoder only LLM\n",
        "device = 'cuda:0'\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-ZqAx9qzDqb"
      },
      "source": [
        "## Greedy Search Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NMtTjm24y_dS"
      },
      "outputs": [],
      "source": [
        "def greedy_decoding(model, text, max_length=10):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # loop until the maximum length is reached\n",
        "        for _ in range(max_length):\n",
        "            # feed X_{1...t} and get token logits for t+1 th step\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # find the most likely token\n",
        "            next_token_id = torch.argmax(logits, dim=-1, keepdim=True).to(device)\n",
        "\n",
        "            # append X_{t+1} to the input sequence\n",
        "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "\n",
        "            # break if <eos> token is generated\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # decode the generated tokens and return the text\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1sx1L2ifHFk",
        "outputId": "5c6c2632-639b-461e-be4c-ea7e37de6eb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It rains a lot in the summer, and the weather is very humid.\n",
            "---\n",
            "Tell me about apples:\n",
            "\n",
            "- Apples are a good source of fiber, vitamin C, and antioxidants.\n",
            "- Apples are a good source of fiber, vitamin C, and antioxidants.\n",
            "- Apples are\n"
          ]
        }
      ],
      "source": [
        "print(greedy_decoding(model, \"It rains a lot in the\"))\n",
        "print(\"---\")\n",
        "print(greedy_decoding(model, \"Tell me about apples:\", 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgUsBVmAa5ku"
      },
      "source": [
        "## Decoding with Sampling: Top K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "482bEmmXSVZK"
      },
      "outputs": [],
      "source": [
        "def sampling_decoding_top_k(model, text, top_k=50, max_length=30):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # loop until the maximum length is reached\n",
        "        for _ in range(max_length):\n",
        "            # feed X_{1...t} and get token logits for t+1 th step\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # find top_k tokens\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            top_k_logits, top_k_indices = sorted_logits[:, :top_k], sorted_indices[:, :top_k]\n",
        "\n",
        "            # redistribute the probability mass using softmax\n",
        "            top_k_probs = torch.softmax(top_k_logits, dim=-1)\n",
        "\n",
        "            # randomly sample a token based on the probability distribution\n",
        "            chosen_idx = torch.multinomial(top_k_probs, num_samples=1).to(device)\n",
        "            next_token_id = top_k_indices.gather(-1, chosen_idx)\n",
        "\n",
        "            # append X_{t+1} to the input sequence\n",
        "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "\n",
        "            # break if <eos> token is generated\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # decode the generated tokens and return the text\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_jZT3WYLRGR",
        "outputId": "77e1096a-2589-42c3-e05f-a425e828efd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It rains a lot in the monsoon season.\n",
            "\n",
            "How to wear traditional clothes in the Indian summer:\n",
            "\n",
            "To protect against the heat in monsoon and sum\n"
          ]
        }
      ],
      "source": [
        "print(sampling_decoding_top_k(model, \"It rains a lot in the\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8iU_HUtbJeE"
      },
      "source": [
        "## Decoding with Sampling: Top P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xCf7xxqpWCt9"
      },
      "outputs": [],
      "source": [
        "def sampling_decoding_top_p(model, text, top_p=0.92, max_length=30):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # loop until the maximum length is reached\n",
        "        for _ in range(max_length):\n",
        "            # feed X_{1...t} and get token logits for t+1 th step\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # find the minimum set of tokens whose cumulative probability is above the threshold\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
        "\n",
        "            cumulative_prob = torch.cumsum(sorted_probs, dim=-1)\n",
        "            top_p_num = (cumulative_prob > top_p).nonzero(as_tuple=True)[1][0].item() + 1\n",
        "\n",
        "            top_p_logits, top_p_indices = sorted_logits[:, :top_p_num], sorted_indices[:, :top_p_num]\n",
        "\n",
        "            # redistribute the probability mass using softmax\n",
        "            top_p_probs = torch.softmax(top_p_logits, dim=-1)\n",
        "\n",
        "            # randomly sample a token based on the probability distribution\n",
        "            chosen_idx = torch.multinomial(top_p_probs, num_samples=1).to(device)\n",
        "            next_token_id = top_p_indices.gather(-1, chosen_idx)\n",
        "\n",
        "            # append X_{t+1} to the input sequence\n",
        "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "\n",
        "            # break if <eos> token is generated\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # decode the generated tokens and return the text\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBf9itTwTywB",
        "outputId": "8d89f388-8ab7-44a6-8b1c-fdf961e3ddf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It rains a lot in the winter and it’s getting worse. The trees\n"
          ]
        }
      ],
      "source": [
        "print(sampling_decoding_top_p(model, \"It rains a lot in the\", 0.82, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJkbsxpFbKon"
      },
      "source": [
        "## Decoding with Sampling: Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v8Jf-rLGXWVE"
      },
      "outputs": [],
      "source": [
        "def sampling_decoding_temperature(model, text, temperature=1, max_length=30):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # loop until the maximum length is reached\n",
        "        for _ in range(max_length):\n",
        "            # feed X_{1...t} and get token logits for t+1 th step\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # apply softmax with temperature\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            # sample from the distribution\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1).to(device)\n",
        "\n",
        "            # append X_{t+1} to the input sequence\n",
        "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "\n",
        "            # break if <eos> token is generated\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # decode the generated tokens and return the text\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWxu2ageYSA1",
        "outputId": "afa2c2dc-06c5-4867-b93d-7517964b672a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It rains a lot in the summer, and the air is cool and moist.\n",
            "---\n",
            "Tell me about apples: there are more apple trees than can possibly deliver food to humans, yet with the selective breeding of the best trees, we have unlimited apple production. However, individual apples only remain good for a while. No matter what stresses they\n"
          ]
        }
      ],
      "source": [
        "print(sampling_decoding_temperature(model, \"It rains a lot in the\", 0.2, 10))\n",
        "print(\"---\")\n",
        "print(sampling_decoding_temperature(model, \"Tell me about apples:\", 1.0, 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-kx4MojcS9G"
      },
      "source": [
        "## Beam Search Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Iw6NgyCUcbsr"
      },
      "outputs": [],
      "source": [
        "def beam_search_decoding(model, text, num_beams=3, max_length=10):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "    # initialize the beams\n",
        "    # list of tuples (token_ids, product of probabilities)\n",
        "    beams = [(input_ids, 1)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            all_candidates = []\n",
        "\n",
        "            for input_ids, prod_prob in beams:\n",
        "                outputs = model(input_ids)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "\n",
        "                # get the probabilities\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "                # select the top num_beams tokens and their probabilities\n",
        "                top_probs, top_token_ids = torch.topk(probs, num_beams, dim=-1)\n",
        "\n",
        "                for i in range(num_beams):\n",
        "                    next_token_id = top_token_ids[0, i].unsqueeze(0).unsqueeze(0).to(device)\n",
        "                    next_prob = top_probs[0, i].item()\n",
        "\n",
        "                    new_input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "                    new_prod_prob = prod_prob * next_prob\n",
        "\n",
        "                    all_candidates.append((new_input_ids, new_prod_prob))\n",
        "\n",
        "            # keep the top num_beams sequences\n",
        "            beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:num_beams]\n",
        "\n",
        "            # break if all sequences in beams end with <eos> token\n",
        "            if all(tokenizer.eos_token_id in beam[0][0] for beam in beams):\n",
        "                break\n",
        "\n",
        "    # decode the best sequence (the one with the highest prod probability)\n",
        "    best_sequence = beams[0][0]\n",
        "    generated_text = tokenizer.decode(best_sequence[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "188R_xVdusBx",
        "outputId": "b0283b5d-fa11-413d-d5c2-bfbe3e5757ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It rains a lot in the Pacific Northwest, so I’m used to\n"
          ]
        }
      ],
      "source": [
        "print(beam_search_decoding(model, \"It rains a lot in the\", 3, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHbnCHbjeK43"
      },
      "source": [
        "*Note: Parts of the code were written with the help of a Generative AI*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16cc667f790b404c998429efd29f3c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a61dfba2af6345abb338698f39921d4b",
            "placeholder": "​",
            "style": "IPY_MODEL_31b7978c8707428684ac4248ff851a81",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "18fd7e2110a548ff9da6ce1f558dcf29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d8c7f728e6a46fc9ec33dc8bcf46403": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b7978c8707428684ac4248ff851a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44c66b45fbc143f69087b4186d707686": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd84083292304a21a053ef1aad860ae1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18fd7e2110a548ff9da6ce1f558dcf29",
            "value": 2
          }
        },
        "7b9fdb818bee4f5f86b2c0e890967622": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a61dfba2af6345abb338698f39921d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3eaa6296a7846f7880c1ad79c49ba50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb953d96938947f59bcedb09d9da265d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3eaa6296a7846f7880c1ad79c49ba50",
            "placeholder": "​",
            "style": "IPY_MODEL_7b9fdb818bee4f5f86b2c0e890967622",
            "value": " 2/2 [00:04&lt;00:00,  2.24s/it]"
          }
        },
        "cd84083292304a21a053ef1aad860ae1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de7c64f08ac44d6c99440b0715946a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16cc667f790b404c998429efd29f3c3b",
              "IPY_MODEL_44c66b45fbc143f69087b4186d707686",
              "IPY_MODEL_bb953d96938947f59bcedb09d9da265d"
            ],
            "layout": "IPY_MODEL_2d8c7f728e6a46fc9ec33dc8bcf46403"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
